{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7fc799c",
   "metadata": {},
   "source": [
    "Código adaptador de:\n",
    "* https://keras.io/examples/vision/image_classification_with_vision_transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a407fee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e3ed1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ce9d221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pasta = \"dataset/\"\n",
    "\n",
    "def files_path04(path):\n",
    "    infos =[]\n",
    "    for p, _, files in os.walk(os.path.abspath(path)):\n",
    "        for file in files:\n",
    "            infos.append(os.path.join(p, file))\n",
    "    return infos\n",
    "\n",
    "dados = files_path04(pasta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3b42ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tipo = [i.split(\"\\\\\")[-3] for i in dados]\n",
    "subgroup = [i.split(\"\\\\\")[-2] for i in dados]\n",
    "filename = [i.split(\"\\\\\")[-1] for i in dados]\n",
    "classe = [0 if x == \"benigno\" else 1 for x in tipo  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6d8b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89ed80ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['filename'] = filename\n",
    "df['subgroup'] = subgroup\n",
    "df['tipo'] = tipo\n",
    "df['classe'] = classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a239b206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>subgroup</th>\n",
       "      <th>tipo</th>\n",
       "      <th>classe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>benigno10.png</td>\n",
       "      <td>1</td>\n",
       "      <td>benigno</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>benigno11.png</td>\n",
       "      <td>1</td>\n",
       "      <td>benigno</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>benigno12.png</td>\n",
       "      <td>1</td>\n",
       "      <td>benigno</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>benigno13.png</td>\n",
       "      <td>1</td>\n",
       "      <td>benigno</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>benigno1046.png</td>\n",
       "      <td>10</td>\n",
       "      <td>benigno</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2687</th>\n",
       "      <td>maligno982.png</td>\n",
       "      <td>98</td>\n",
       "      <td>maligno</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2688</th>\n",
       "      <td>maligno983.png</td>\n",
       "      <td>98</td>\n",
       "      <td>maligno</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2689</th>\n",
       "      <td>maligno990.png</td>\n",
       "      <td>99</td>\n",
       "      <td>maligno</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2690</th>\n",
       "      <td>maligno991.png</td>\n",
       "      <td>99</td>\n",
       "      <td>maligno</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2691</th>\n",
       "      <td>maligno992.png</td>\n",
       "      <td>99</td>\n",
       "      <td>maligno</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2692 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             filename subgroup     tipo  classe\n",
       "0       benigno10.png        1  benigno       0\n",
       "1       benigno11.png        1  benigno       0\n",
       "2       benigno12.png        1  benigno       0\n",
       "3       benigno13.png        1  benigno       0\n",
       "4     benigno1046.png       10  benigno       0\n",
       "...               ...      ...      ...     ...\n",
       "2687   maligno982.png       98  maligno       1\n",
       "2688   maligno983.png       98  maligno       1\n",
       "2689   maligno990.png       99  maligno       1\n",
       "2690   maligno991.png       99  maligno       1\n",
       "2691   maligno992.png       99  maligno       1\n",
       "\n",
       "[2692 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1623e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "benignos_sub = df[df['tipo']=='benigno']['subgroup'].unique()\n",
    "malignos_sub = df[df['tipo']=='maligno']['subgroup'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a3b162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b622280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(benignos_sub)\n",
    "random.shuffle(malignos_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be01275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(lista, folds):\n",
    "    index=[]\n",
    "    pivo = int(len(lista)/folds)\n",
    "    for i in range(folds+1):\n",
    "        index.append(lista[i*pivo:pivo*(i+1)])\n",
    "    return index\n",
    "\n",
    "folds = 14\n",
    "index_benignos = split(benignos_sub,folds)\n",
    "index_malignos = split(malignos_sub,folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9aeff4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 64\n",
    "height = 64\n",
    "IMAGE_SIZE = (width,height)\n",
    "num_densa = 256\n",
    "channels = 3\n",
    "num_out = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e514236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "124d9aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7ffb4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def images_labels(indexs,tipo):\n",
    "    first = True\n",
    "    for i in indexs:\n",
    "\n",
    "        imagens = df[(df['subgroup']==i)& (df['tipo']==tipo)]['filename'].values\n",
    "        X_train = np.zeros((len(imagens), width, height, channels))\n",
    "\n",
    "        for idx,j in  enumerate(imagens):\n",
    "            X_train[idx] =  np.asarray(cv2.resize(cv2.imread(pasta+tipo+\"/\"+i+\"/\"+j), (64,64), interpolation = cv2.INTER_AREA))\n",
    "\n",
    "        X_train = X_train.reshape(( X_train.shape[0],) +(width, height,channels)).astype('float32') / 255. \n",
    "        if first:\n",
    "            out = X_train\n",
    "        else:\n",
    "            out = np.concatenate((X_train,out))  \n",
    "        first = False\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4f86886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_treino_idx(accessed_list):\n",
    "    out =[]\n",
    "    for i in accessed_list:\n",
    "        for j in i:\n",
    "            out.append(j)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16d5b413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_por_fold(index_benignos,treino_idx,teste_idx,val_idx,tipo):\n",
    "    treino_idx.remove(val_idx)\n",
    "    treino_idx.remove(teste_idx)\n",
    "    accessed_mapping = map(index_benignos.__getitem__, treino_idx)\n",
    "    accessed_list = list(accessed_mapping)\n",
    "    treino_idx = concat_treino_idx(accessed_list)\n",
    "    \n",
    "    treino = images_labels(treino_idx,tipo)\n",
    "    val = images_labels(index_benignos[val_idx],tipo)\n",
    "    teste = images_labels(index_benignos[teste_idx],tipo)\n",
    "    return treino,val,teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "079135c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d934cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import efficientnet\n",
    "\n",
    "def get_cnn_efficientnet():\n",
    "    base_model = efficientnet.EfficientNetB0(\n",
    "        input_shape=(*IMAGE_SIZE, channels), include_top=False,\n",
    "    )\n",
    "\n",
    "    base_model.trainable = True\n",
    "    base_model_out = base_model.output\n",
    "    base_model_out = layers.Flatten()(base_model_out)\n",
    "    base_model_out = layers.Dense(num_densa, activation=\"relu\")(base_model_out)\n",
    "    base_model_out = layers.Dense(num_out, activation=\"sigmoid\")(base_model_out)\n",
    "    cnn_model = keras.models.Model(base_model.input, base_model_out)\n",
    "    return cnn_model\n",
    "\n",
    "\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "\n",
    "def get_cnn_VGG19():\n",
    "    base_model = VGG19(\n",
    "        input_shape=(*IMAGE_SIZE, channels), include_top=False,\n",
    "    )\n",
    "\n",
    "    base_model.trainable = True\n",
    "    base_model_out = base_model.output\n",
    "    base_model_out = layers.Flatten()(base_model_out)\n",
    "    base_model_out = layers.Dense(num_densa, activation=\"relu\")(base_model_out)\n",
    "    base_model_out = layers.Dense(num_out, activation=\"sigmoid\")(base_model_out)\n",
    "    cnn_model = keras.models.Model(base_model.input, base_model_out)\n",
    "    return cnn_model\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "\n",
    "def get_cnn_ResNet50():\n",
    "    base_model = ResNet50(\n",
    "        input_shape=(*IMAGE_SIZE, channels), include_top=False,\n",
    "    )\n",
    "\n",
    "    base_model.trainable = True\n",
    "    base_model_out = base_model.output\n",
    "    base_model_out = layers.Flatten()(base_model_out)\n",
    "    base_model_out = layers.Dense(num_densa, activation=\"relu\")(base_model_out)\n",
    "    base_model_out = layers.Dense(num_out, activation=\"sigmoid\")(base_model_out)\n",
    "    cnn_model = keras.models.Model(base_model.input, base_model_out)\n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ffc818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4b14081",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_idx = 1\n",
    "teste_idx = 2\n",
    "\n",
    "for i in range(folds):\n",
    "    #ler dados\n",
    "    tipo = \"benigno\"\n",
    "    treino_idx = [item for item in range(folds)]\n",
    "    treino_b,val_b,teste_b = images_por_fold(index_benignos,treino_idx,teste_idx,val_idx,tipo)\n",
    "    treino_b_label = np.repeat(0, len(treino_b))\n",
    "    val_b_label = np.repeat(0, len(val_b))\n",
    "    teste_b_label = np.repeat(0, len(teste_b))\n",
    "    \n",
    "    tipo = \"maligno\"\n",
    "    treino_idx = [item for item in range(folds)]\n",
    "    treino_m,val_m,teste_m = images_por_fold(index_malignos,treino_idx,teste_idx,val_idx,tipo)\n",
    "    treino_m_label = np.repeat(1, len(treino_m))\n",
    "    val_m_label = np.repeat(1, len(val_m))\n",
    "    teste_m_label = np.repeat(1, len(teste_m))\n",
    "    \n",
    "    X_train = np.concatenate((treino_b,treino_m))\n",
    "    y_train = np.concatenate((treino_b_label,treino_m_label))\n",
    "    \n",
    "    X_val = np.concatenate((val_b,val_m))\n",
    "    y_val = np.concatenate((val_b_label,val_m_label))\n",
    "    \n",
    "    #Cortando o malignos para ficar igual\n",
    "    X_teste = np.concatenate((teste_b,teste_m[:len(teste_b)]))\n",
    "    y_teste = np.concatenate((teste_b_label,teste_m_label[:len(teste_b_label)]))\n",
    "    \n",
    "    datagen = ImageDataGenerator(horizontal_flip=True,rotation_range=30,zoom_range=[0.1,1.0])\n",
    "    it = datagen.flow(X_train, y_train, batch_size=10000)\n",
    "    data = next(it)\n",
    "    \n",
    "    X_train = np.concatenate((X_train,data[0]))\n",
    "    y_train = np.concatenate((y_train,data[1]))\n",
    "    \n",
    "    \n",
    "    #execução modelos um aqui\n",
    "\n",
    "    #função do calculo das métricas\n",
    "    \n",
    "    \n",
    "    #update folds\n",
    "\n",
    "    val_idx = val_idx + 1\n",
    "    if val_idx> folds:\n",
    "        val_idx = 0\n",
    "    teste_idx = teste_idx + 1\n",
    "    if teste_idx> folds:\n",
    "        teste_idx = 0\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f10c387",
   "metadata": {},
   "source": [
    "# VIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1135939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "input_shape = (64, 64, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30cfff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 16\n",
    "num_epochs = 100\n",
    "image_size = 64  # We'll resize input images to this size\n",
    "patch_size = 6  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d23d8727",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b267dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c27dc519",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "939ee4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b10cedd5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 64 X 64\n",
      "Patch size: 6 X 6\n",
      "Patches per image: 100\n",
      "Elements per patch: 108\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 100 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "image = X_train[np.random.choice(range(X_train.shape[0]))]\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "resized_image = tf.image.resize(\n",
    "    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
    ")\n",
    "patches = Patches(patch_size)(resized_image)\n",
    "print(f\"Image size: {image_size} X {image_size}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
    "print(f\"Patches per image: {patches.shape[1]}\")\n",
    "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
    "\n",
    "n = int(np.sqrt(patches.shape[1]))\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i, patch in enumerate(patches[0]):\n",
    "    ax = plt.subplot(n, n, i + 1)\n",
    "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
    "    plt.imshow(patch_img.numpy())\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b44b6739",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77b844ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(1,activation=\"sigmoid\")(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5a65623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model):\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "\n",
    "   # checkpoint_filepath = \"/tmp/checkpoint\"\n",
    "    #checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "     #   checkpoint_filepath,\n",
    "      #  monitor=\"val_accuracy\",\n",
    "       # save_best_only=True,\n",
    "        #save_weights_only=True,\n",
    "   # )\n",
    "\n",
    "    history = model.fit(\n",
    "        x=X_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=(X_val, y_val),\n",
    "        #validation_split=0.1,\n",
    "        callbacks=[es],\n",
    "    )\n",
    "\n",
    "  #  model.load_weights(checkpoint_filepath)\n",
    "   # _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    #print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    #print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eba8d2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "18/18 [==============================] - 12s 266ms/step - loss: 1.5338 - accuracy: 0.6645 - val_loss: 0.6248 - val_accuracy: 0.6154\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 4s 210ms/step - loss: 0.6105 - accuracy: 0.7184 - val_loss: 0.5540 - val_accuracy: 0.8242\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 4s 209ms/step - loss: 0.5824 - accuracy: 0.7252 - val_loss: 0.5632 - val_accuracy: 0.7527\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 4s 207ms/step - loss: 0.5815 - accuracy: 0.7301 - val_loss: 0.5640 - val_accuracy: 0.5714\n",
      "Epoch 4: early stopping\n"
     ]
    }
   ],
   "source": [
    "vit_classifier = create_vit_classifier()\n",
    "history = run_experiment(vit_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b3a30c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da6aeaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Benigno       0.82      0.26      0.39        54\n",
      "     Maligno       0.56      0.94      0.70        54\n",
      "\n",
      "    accuracy                           0.60       108\n",
      "   macro avg       0.69      0.60      0.55       108\n",
      "weighted avg       0.69      0.60      0.55       108\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVoAAAD4CAYAAACt8i4nAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAPzklEQVR4nO3de5CddX3H8fd3L0mMckmApFuk3IwXLopjdLRQxxIGqFoT24ERe9lqxm3HolgdW4qtDnaqtI5Isa3OVtBtLWhQkFQsiIuICkripQgERBkISCSWcA9Eds+3f+wxs81lz9nk/M5z8uT9yjxzzvOcPc/5Zmbz4cv3uZzITCRJ5fRVXYAk1Z1BK0mFGbSSVJhBK0mFGbSSVNhA6Q8449AVntag7Vz07l+rugT1oPlnfTJ2dx/P/O/dbWfO4IFH7PbntaN40EpSVzUmq65gOwatpHrJRsd2FRH3AI8Dk8BEZi6NiIXA54HDgHuA0zPz4Zn244xWUr00Gu0v7fntzDwuM5c2188GxjNzCTDeXJ+RQSupVjIbbS+7aDkw1nw+Bqxo9QaDVlK9TE60vUTESESsnbaMbLO3BL4aEd+b9trizNwA0Hxc1KokZ7SS6mUWB8MycxQYneFHjs/MByJiEXBtRNyxKyXZ0Uqql2y0v7TaVeYDzceNwBXAK4AHI2IIoPm4sdV+DFpJ9dKhg2ER8eyI2OdXz4GTgVuB1cBw88eGgStbleToQFKt7MZBrm0tBq6ICJjKyksy8+qIWAOsioiVwHrgtFY7Mmgl1Uv7p23NKDPvBl6yg+0PActmsy+DVlK9TD5TdQXbMWgl1UsHrwzrFINWUr10aHTQSQatpHqxo5WkwuxoJamsbHgwTJLKsqOVpMKc0UpSYX7DgiQVZkcrSYU5o5WkwiYnqq5gOwatpHqxo5WksjI9GCZJZdnRSlJhnnUgSYXZ0UpSYZ51IEmFOTqQpMIcHUhSYQatJBXm6ECSCvNgmCQV5uhAkgpzdCBJhdnRSlJhBq0kFZZZdQXbMWgl1cuEZx1IUlkeDJOkwpzRSlJhzmglqTA7WkkqzKCVpLJy0i9nlKSyerCj7au6AEnqqGy0v7QhIvoj4gcR8eXm+sKIuDYi7mo+Lmi1D4NWUr00sv2lPWcB66atnw2MZ+YSYLy5PiODVlK9NBrtLy1ExHOB1wGfmrZ5OTDWfD4GrGi1H2e0kuplFgfDImIEGJm2aTQzR6etXwD8JbDPtG2LM3MDQGZuiIhFrT7HjraQP/3ImXzye5/hH7/6T9u99rqR5Vx675fYZ8E+O3inai+CeWecw9w3vH1qfe585r7xLOYNf5C5bzwL5s6vtr493Sw62swczcyl05atIRsRrwc2Zub3drckg7aQb1x2HecNf3C77QuHDuTYE47jF/dvrKAq9YKB406k8fDPt64PLj2Vyfvu4Omx9zN53x0MLj2lwupqoHMz2uOBN0TEPcDngBMj4rPAgxExBNB8bPmP2aAt5I6bb+eJR57Ybvsfv/+tXPLhMei9qwTVBfGc/ek//Fgmbv321m39R76YidtvAmDi9pvoP/IlVZVXDx066yAz/zozn5uZhwFvAq7LzD8EVgPDzR8bBq5sVVLLGW1EvJCp4e/BTMXDA8DqzFw34xu1nZed9HI2/fwh1q+7p+pSVJHBV5/OL791OTE4b+u2mL8vbH5samXzY8SzHCntlvbPJthV5wGrImIlsB44rdUbZuxoI+KvmGqZA7gZWNN8fmlE7PSUhogYiYi1EbH2J0/c0375NTZn3hxWnHkal51/adWlqCJ9hx9LPvU4uXF91aXUWjYabS9t7zPz+sx8ffP5Q5m5LDOXNB83tXp/q452JXB0Zj4zfWNEnA/cxlSy76ioUWAU4IxDV/g/ycDiQ4c46JBF/MN/XwDAwqED+NBV5/M3y9/Lo794pNLa1B39Q0fSf/iL6T/sGKJ/AOY8izmnvIXc/Bj8qqudvy/51ONVl7pn2wMvwW0Avw7cu832oeZratN9d97Ln73sT7auX/itUd73u+/h8Yf9R7W3eObGL/HMjV8CoO/g5zP4spP45TWfZvCE32PgqFcxsfYaBo56FZM/vaXaQvd05UcHs9YqaN8FjEfEXcB9zW2/ATwPOLNgXXu8d1z4bl70qmPYZ8G+/PN3PsUXPvY5rv/816ouSz3ombXXMPe1b2Pg6OPJxzex5arR1m/SzvXgvQ5mDNrMvDoing+8gqmDYQHcD6zJzN7rz3vIx995/oyvv/OEkRlfV701fvZjtvzsx1MrTz/JlssvqLSeWtkDO1oyswF8pwu1SNLu8zvDJKmwPbGjlaQ9SU703lTToJVUL3a0klSYM1pJKsyOVpLKSoNWkgrzYJgkFWZHK0mFGbSSVFamQStJZdnRSlJhBq0klZUTXrAgSWX1Xs4atJLqxQsWJKk0g1aSCnN0IEllOTqQpMJywqCVpLIcHUhSWT1432+DVlLNGLSSVJYdrSQVlhNVV7A9g1ZSrdjRSlJhBq0klZZRdQXbMWgl1YodrSQVlg07WkkqqjFp0EpSUb04OuirugBJ6qRsRNvLTCJiXkTcHBH/ExG3RcS5ze0LI+LaiLir+bigVU0GraRayWx/aWELcGJmvgQ4Djg1Il4JnA2MZ+YSYLy5PiODVlKtdKqjzSlPNFcHm0sCy4Gx5vYxYEWrmgxaSbXSmIy2l4gYiYi105aR6fuKiP6I+CGwEbg2M78LLM7MDQDNx0WtavJgmKRamc3pXZk5CozO8PokcFxE7A9cERHH7EpNdrSSaiUz2l7a32c+AlwPnAo8GBFDAM3Hja3eb9BKqpVstL/MJCIOanayRMSzgJOAO4DVwHDzx4aBK1vV5OhAUq00OnevgyFgLCL6mWpKV2XmlyPiJmBVRKwE1gOntdqRQSupVmYzEph5P3kL8NIdbH8IWDabfRm0kmrFS3AlqTBvKiNJhXVwRtsxBq2kWunUjLaTDFpJtdLGPQy6zqCVVCuODiSpsIYHwySprL2yo71sw5rSH6E90L//wTerLkE15cEwSSpsr+xoJambevCkA4NWUr1MNnrvpoQGraRa6cEvwTVoJdVL4oxWkopq9OCQ1qCVVCsNO1pJKsvRgSQVNmnQSlJZnnUgSYUZtJJUmDNaSSqsB++SaNBKqhdP75KkwiarLmAHDFpJtdIIO1pJKqoHr8A1aCXVi6d3SVJhnnUgSYV5Ca4kFWZHK0mFOaOVpMI860CSCnN0IEmFOTqQpMIm7WglqSw7WkkqrBeDtq/qAiSpk3IWy0wi4pCI+HpErIuI2yLirOb2hRFxbUTc1Xxc0Komg1ZSrTSi/aWFCeA9mfki4JXAn0fEUcDZwHhmLgHGm+szMmgl1UpjFstMMnNDZn6/+fxxYB1wMLAcGGv+2BiwolVNBq2kWpmcxRIRIxGxdtoysqN9RsRhwEuB7wKLM3MDTIUxsKhVTR4Mk1Qrs7lgITNHgdGZfiYingN8EXhXZj4Wu3BjcTtaSbXSqdEBQEQMMhWy/5mZlzc3PxgRQ83Xh4CNrfZj0EqqlQ6edRDARcC6zDx/2kurgeHm82HgylY1OTqQVCuNzt1W5njgj4AfRcQPm9vOAc4DVkXESmA9cFqrHRm0kmqlU9+Cm5nfgp3eRXzZbPZl0EqqlV68MsyglVQr3iZRkgrr4Iy2YwxaSbXSezFr0EqqGWe0klTYZA/2tAatpFqxo5WkwjwYJkmF9V7MGrSSasbRgSQV5sEwSSrMGe1eau7cuVx/3ReZM3cuAwP9XH75VZz7wY9WXZYqcvLvD/Ps+fPp6+ujv7+fVRdfyDXXfZN/veiz3H3vfVz6bxdwzIueX3WZe6zei1mDtiu2bNnCSSefzpNPbmZgYIAbrr+Cq6/+Ot+9+ftVl6aKXPzx81iw/35b1593xKFc8KG/5dyPXFhhVfVgR7sXe/LJzQAMDg4wMDhIZu/9Mqg6Rx72G1WXUBu9eDDMb1jokr6+Ptau+SobfnYL4+M3cPOaH1RdkioSEYz8xfs4/a3v4LIrv1J1ObWTs/jTLbvc0UbEWzLz0zt5bQQYAYj+/ejre/aufkxtNBoNlr78ZPbbb1++eNlFHH30C7jttjurLksV+I9PfJRFBx3AQw8/wtvedQ6HH3oIS487tuqyaqMXzzrYnY723J29kJmjmbk0M5casv/fo48+xjduuJFTTn5N1aWoIosOOgCAAxbsz7JX/yY/ut3/4HZSJ7+csVNmDNqIuGUny4+AxV2qcY934IEL2W+/fQGYN28ey078Le6886cVV6UqbH7q6a3z+s1PPc2NN3+fJUccVm1RNdPIbHvpllajg8XAKcDD22wP4MYiFdXQ0NBiLr7oAvr7++jr6+MLX/gvrvrK16ouSxV4aNPDnHXO3wEwOTHJa09+DSe8cilf+8a3+fDHPsGmRx7l7e/9AC9ccgSjH/v7iqvdM/Xe4ABipqPfEXER8Onml5Rt+9olmfnmVh8wMOfgXvx7q2JPPfDNqktQDxo88Ijd/iKaNx/6xrYz55J7r+jKF9/M2NFm5soZXmsZspLUbd08m6BdnkcrqVYmDFpJKsuOVpIK68UrwwxaSbXSi5e3G7SSasWbykhSYb14Ca5BK6lW7GglqTBntJJUmGcdSFJhnkcrSYU5o5Wkwiaz94YHBq2kWnF0IEmFdfOG3u3yyxkl1UrOYmklIi6OiI0Rceu0bQsj4tqIuKv5uKDVfgxaSbXSINte2vAZ4NRttp0NjGfmEmC8uT4jg1ZSrXQyaDPzBmDTNpuXA2PN52PAilb7cUYrqVZmc9ZBRIwAI9M2jWbmaIu3Lc7MDQCZuSEiFrX6HINWUq3M5qyDZqi2CtbdZtBKqpUu3OvgwYgYanazQ8DGVm9wRiupVjp8MGxHVgPDzefDwJWt3mBHK6lWOtnRRsSlwGuAAyPifuADwHnAqohYCawHTmu1H4NWUq1MdvD+XZl5xk5eWjab/Ri0kmqlF68MM2gl1Yr3OpCkwuxoJakwO1pJKsyOVpIK88bfklSYowNJKiztaCWpLL+cUZIK68JNZWbNoJVUK3a0klTYZMMZrSQV5VkHklSYM1pJKswZrSQVZkcrSYV5MEySCnN0IEmFOTqQpMK8TaIkFeZ5tJJUmB2tJBXW8DaJklSWB8MkqTCDVpIK672YhejF9K+riBjJzNGq61Bv8fei/vqqLmAvM1J1AepJ/l7UnEErSYUZtJJUmEHbXc7htCP+XtScB8MkqTA7WkkqzKCVpMIM2i6JiFMj4s6I+ElEnF11PapeRFwcERsj4taqa1FZBm0XREQ/8C/A7wBHAWdExFHVVqUe8Bng1KqLUHkGbXe8AvhJZt6dmb8EPgcsr7gmVSwzbwA2VV2HyjNou+Ng4L5p6/c3t0naCxi03RE72OZ5ddJewqDtjvuBQ6atPxd4oKJaJHWZQdsda4AlEXF4RMwB3gSsrrgmSV1i0HZBZk4AZwLXAOuAVZl5W7VVqWoRcSlwE/CCiLg/IlZWXZPK8BJcSSrMjlaSCjNoJakwg1aSCjNoJakwg1aSCjNoJakwg1aSCvs/iDkXI6bSQTUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "threshold =0.5\n",
    "y_pred = np.where(vit_classifier.predict(X_teste)> threshold, 1,0)\n",
    "target_names = ['Benigno', \"Maligno\"]\n",
    "print(classification_report(y_teste, y_pred, target_names=target_names))\n",
    "cf_matrix = confusion_matrix(y_teste, y_pred)\n",
    "import seaborn as sns\n",
    "sns.heatmap(cf_matrix, annot=True,fmt=\"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fffd131",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
